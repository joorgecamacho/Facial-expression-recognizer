{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160d8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c00d1",
   "metadata": {},
   "source": [
    "I am going to build the function to get face landmarks individually from each image. It will return a list with the normalized landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a148c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_landmarks(image, static_image_mode):\n",
    "    single_landmarks = []\n",
    "    \n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode, \n",
    "                                                max_num_faces=1, min_detection_confidence=0.5)\n",
    "    results = face_mesh.process(rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        single_face = results.multi_face_landmarks[0].landmark\n",
    "        xs_ = []\n",
    "        ys_ = []\n",
    "        zs_ = []\n",
    "        for idx in single_face:\n",
    "            xs_.append(idx.x)\n",
    "            ys_.append(idx.y)\n",
    "            zs_.append(idx.z)\n",
    "        for j in range(len(xs_)):\n",
    "            single_landmarks.append(xs_[j] - min(xs_))\n",
    "            single_landmarks.append(ys_[j] - min(ys_))\n",
    "            single_landmarks.append(zs_[j] - min(zs_))\n",
    "    return single_landmarks if single_landmarks else [0] * 468 * 3  # Return a list of zeros if no landmarks are detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2922d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(directory):\n",
    "    # Define the emotion folder names\n",
    "    emotion_folders = [\"Happy\", \"Neutral\", \"Surprised\"]\n",
    "    lmrk = []\n",
    "    array = []\n",
    "    # Iterate over the root folder \"final_dataset\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for idx, emotion_folder in enumerate(emotion_folders, start=1):\n",
    "            # Check if the current directory is one of the emotion folders\n",
    "            if emotion_folder in dirs:\n",
    "                emotion_folder_path = os.path.join(root, emotion_folder)\n",
    "                # Iterate through the files in the emotion folder\n",
    "                for file_name in os.listdir(emotion_folder_path):\n",
    "                    # Construct the full path of the image file\n",
    "                    image_path = os.path.join(emotion_folder_path, file_name)\n",
    "                    image = cv2.imread(image_path)\n",
    "                    # Append the image path along with its emotion index to the list\n",
    "                    lmrk = single_landmarks(image, True)\n",
    "                    lmrk.append(int(idx))\n",
    "                    array.append(lmrk)\n",
    "                    \n",
    "    np.savetxt('data.txt', np.asarray(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ed08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_search = r\"C:\\Users\\Jorge\\Facial expression recognizer\\Final_dataset\"\n",
    "prepare_data(folder_to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed276d78",
   "metadata": {},
   "source": [
    "Now we have already prepared the data that will rain our model. It consists on all teh landmarks of the faces in our images, related to an index, which shows which expression that image has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed9859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.87%\n",
      "[[74  1  6]\n",
      " [ 2 64  3]\n",
      " [ 7  4 66]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load data from the text file\n",
    "data_file = \"data.txt\"\n",
    "data = np.loadtxt(data_file)\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data[:, :-1]  # Features are all columns except the last one\n",
    "y = data[:, -1]   # Labels are the last column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "with open('./model', 'wb') as f:\n",
    "    pickle.dump(rf_classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098195e2",
   "metadata": {},
   "source": [
    "We can see that the model is well trained as it is having a great accuracy and the confussion matrix is showing few confussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1350818",
   "metadata": {},
   "source": [
    "Testing the model in real time images with my face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b728868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n",
      "Surprised\n"
     ]
    }
   ],
   "source": [
    "def test_model(frame):\n",
    "    with open('./model', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    face_landmarks = single_landmarks(frame, False)\n",
    "    output = model.predict([face_landmarks])\n",
    "\n",
    "    if output == 1: print(\"Happy\")\n",
    "    elif output == 2:print(\"Neutral\")\n",
    "    else:print(\"Surprised\")\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = str(output)\n",
    "    position = (50, 50)\n",
    "    font_scale = 1\n",
    "    font_color = (255, 255, 255)  # white color in BGR\n",
    "    thickness = 2\n",
    "    cv2.putText(frame, text, position, font, font_scale, font_color, thickness)\n",
    "    \n",
    "    \n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.waitKey(25)\n",
    "\n",
    "def start():\n",
    "    \n",
    "    #Videocapture, select yopur camera with 0,1,2,3...\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    #check the access to camera\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open camera.\")\n",
    "        exit()\n",
    "\n",
    "    #capture each frame until pressed \"q\"\n",
    "    \n",
    "    while True:\n",
    "        ret, f = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read frame.\")\n",
    "            break\n",
    "       \n",
    "        \n",
    "        test_model(f)\n",
    "\n",
    "        # Check for keypress (press 'q' to exit)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "    #release the video and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c544a",
   "metadata": {},
   "source": [
    "Clearly it is not working despite of having seen that our model was pretty accurate. Maybe the problem is the dataset, that is not useful for real time imagess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
